{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Public] Prompt Parrot v1.2",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelPaulukonis/notebooks/blob/main/%5BPublic%5D_Prompt_Parrot_v1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Prompt Parrot! \n",
        "\n",
        "This notebook is designed to train GPT-2 on a list of your prompts. And then parrot your prompts back at you. They could be brilliant. They could be hilarious. Either way, it'll be fun!\n",
        "\n",
        "This notebook created and maintained by [Stephen Young](https://twitter.com/KyrickYoung) or SteveTheNinja#0616 on discord\n",
        "\n",
        "# Instructions\n",
        "\n",
        "1. Create a text file with all of your prompts!\n",
        "\n",
        "    **Prompt Parrot requires a minimum of 15-20 prompts to work.** However more than 50 prompts is ideal! The more the merrier.\n",
        "\n",
        "    Example prompt file:\n",
        "    ```\n",
        "    a beautiful painting of a mountain by Tyler Edlin, trending on arstation\n",
        "    a stunning painting of Chicago, oil and canvas\n",
        "    cyberpunk hoover dam, cgsociety, artstation, highly detailed matte painting\n",
        "    ```\n",
        "\n",
        "2. Next upload your file to the session like so\n",
        "\n",
        "    ![upload_icon.png](https://drive.google.com/uc?id=14Is1KFTqpRwqeCWuIPC3StQsAbmG6als)\n",
        "\n",
        "3. Finally copy the path to your uploaded file and paste it below!\n",
        "\n",
        "    ![upload_icon.png](https://drive.google.com/uc?id=1sjvjMc3mUkmbNgh3SkRd616A_Gvol5zP)"
      ],
      "metadata": {
        "id": "eQ5t_q6jT9j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Setup"
      ],
      "metadata": {
        "id": "WvAMD91kKAMC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y8dzG-cHQRw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 0.1 GPU Check\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0.2 Install Dependencies\n",
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "rtFk4O4gS6vx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 0.3 Imports\n",
        "\n",
        "import random\n",
        "import os\n",
        "import random\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TextDataset, DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "import datasets\n",
        "from google.colab import files\n",
        "\n",
        "# no need for caching with tiny datasets. More trouble than it's worth\n",
        "datasets.disable_caching()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fD_VB5uxMOh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Do the run\n",
        "\n"
      ],
      "metadata": {
        "id": "hiewVtvTG0r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.1 Prompts.txt\n",
        "\n",
        "prompts_file_path = \"/content/prompts.txt\" #@param{type:\"string\"}\n",
        "\n",
        "all_prompts = []\n",
        "if prompts_file_path:\n",
        "    with open(prompts_file_path) as infile:\n",
        "        all_prompts = infile.read().strip().split(\"\\n\")\n",
        "else:\n",
        "    uploaded = files.upload()\n",
        "    _, all_prompts = list(uploaded.items())[0]\n",
        "    all_prompts = all_prompts.decode(\"UTF-8\").split(\"\\n\")\n",
        "\n",
        "if not all_prompts:\n",
        "    raise UserWarning(f\"Read 0 prompts from {prompts_file_path}\")\n",
        "\n",
        "prompt_starts = list(set([\" \".join(p.split()[0:2]).replace(\",\", \"\") for p in all_prompts if len(p.split()) > 1]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "775w0YYfMk9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2 Train GPT-2\n",
        "#@markdown Number of iterations to train for. Try 50-500? If prompts generated are identical to your input prompts, try turning down num_train_epochs.\n",
        "num_train_epochs=50 #@param{type:\"integer\"}\n",
        "\n",
        "end_token = \"<|endoftext|>\"\n",
        "prompts_txt = \"scrambled_prompts.txt\"\n",
        "\n",
        "# scramble the prompts so the model doesn't learn association between lines\n",
        "with open(prompts_txt, \"w+\") as fp:\n",
        "    for _ in range(4):\n",
        "        random.shuffle(all_prompts)\n",
        "        fp.write(end_token.join(all_prompts) + end_token)\n",
        "\n",
        "# quick and dirty workaround to blow away the cache for now\n",
        "# TODO: upgrade to huggingface datasets lib. TextDataset is deprecated\n",
        "!rm /content/cached_lm_GPT2TokenizerFast*\n",
        "!rm -rf /content/output/\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=prompts_txt, block_size=tokenizer.model_max_length)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=1,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=100,\n",
        "    save_steps=0,\n",
        "    seed=random.randint(0, 2**32-1),\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "# model.save_pretrained(\"./model\")"
      ],
      "metadata": {
        "id": "n_v5YmzzSj00",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.3 Generate prompts!\n",
        "\n",
        "#@markdown _What?! A prompt for my prompts?!?_ **PREPOSTEROUS!!** \n",
        "#@markdown\n",
        "#@markdown GPT-2 requires a prompt string to start a sentence. Best results will resemble part of your prompts. It can be a single letter or a full sentence.\n",
        "#@markdown `If you leave this blank, Prompt Parrot will automatically choose starting phrases from your prompts`\n",
        "prompt_override=\"bugs\" #@param{type:\"string\"}\n",
        "num_prompts=15 #@param{type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown Maximum and min length of the generated prompts. Will cut off mid word. This is expected behavior\n",
        "max_length=60 #@param{type:\"integer\"}\n",
        "min_length=10 #@param{type:\"integer\"}\n",
        "#@markdown `temperature`: If you find your prompts are too similar to inputs, try turning up the temperature. If your prompts are too insane, turn down the temperature. A good deafult is 1.6\n",
        "temperature=2.0 #@param{type:\"number\"}\n",
        "#@markdown `top_k`: If you find your prompts are identical to your inputs, try turning down top_k. A good range is 70-100. A good default is 100.\n",
        "top_k=70 #@param{type:\"integer\"}\n",
        "top_p=0.9  #@param{type:\"number\"}\n",
        "\n",
        "prompt = random.choice(prompt_starts)\n",
        "if prompt_override:\n",
        "    prompt = prompt_override\n",
        "\n",
        "encoded_prompt = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "encoded_prompt = encoded_prompt.to(model.device)\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    max_length=max_length,\n",
        "    min_length=min_length,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=num_prompts,\n",
        "    pad_token_id=tokenizer.eos_token_id # gets rid of warning\n",
        "    )\n",
        "\n",
        "for generated_sequence in output_sequences:\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "    print(text.strip().replace(\"\\n\", \" \").replace(\"/\", \",\"))"
      ],
      "metadata": {
        "id": "k0I6XNP0fzUJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}