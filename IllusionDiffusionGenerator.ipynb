{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "IllusionDiffusionGenerator.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelPaulukonis/notebooks/blob/main/IllusionDiffusionGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to use this Illusion Diffusion Generator\n",
        "Credits to the original app at [HugginFace](https://huggingface.co/spaces/AP123/IllusionDiffusion)\n",
        "\n",
        "To use this notebook, click the play button next to each section.\n",
        "1. The first one is to get all the packages, models and defining functions later used in the notebook.\n",
        "2. The second section is where you upload your original image.\n",
        "3. In this part if you dont want to meess around the settings just set the `prompt` to the image you want to generate and leave the rest as it is.\n",
        "4. Finally the last section will generate the image."
      ],
      "metadata": {
        "id": "ZfkrLgSKUPV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Installing and Importing the Packages. Then Function Definitions\n",
        "# @markdown ###Will take some time to run. So please be patient.\n",
        "!pip install diffusers --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install xformers --quiet\n",
        "!pip install gradio --quiet\n",
        "!pip install Pillow --quiet\n",
        "!pip install qrcode --quiet\n",
        "!pip install filelock --quiet\n",
        "!pip install https://gradio-builds.s3.amazonaws.com/52ceac5ecd12fa0990273dcb69c899abfb9c6a27/gradio-3.45.1-py3-none-any.whl --quiet\n",
        "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu118 --quiet\n",
        "\n",
        "import torch\n",
        "import gradio as gr\n",
        "from gradio import processing_utils, utils\n",
        "from PIL import Image\n",
        "import random\n",
        "from diffusers import (\n",
        "    DiffusionPipeline,\n",
        "    AutoencoderKL,\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    ControlNetModel,\n",
        "    StableDiffusionLatentUpscalePipeline,\n",
        "    StableDiffusionImg2ImgPipeline,\n",
        "    StableDiffusionControlNetImg2ImgPipeline,\n",
        "    DPMSolverMultistepScheduler,  # <-- Added import\n",
        "    EulerDiscreteScheduler  # <-- Added import\n",
        ")\n",
        "import time\n",
        "\n",
        "BASE_MODEL = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\n",
        "#init_pipe = DiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V5.1_noVAE\", torch_dtype=torch.float16)\n",
        "controlnet = ControlNetModel.from_pretrained(\"monster-labs/control_v1p_sd15_qrcode_monster\", torch_dtype=torch.float16)#, torch_dtype=torch.float16)\n",
        "main_pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    controlnet=controlnet,\n",
        "    vae=vae,\n",
        "    safety_checker=None,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(\"cuda\")\n",
        "\n",
        "image_pipe = StableDiffusionControlNetImg2ImgPipeline(**main_pipe.components)\n",
        "\n",
        "SAMPLER_MAP = {\n",
        "    \"DPM++ Karras SDE\": lambda config: DPMSolverMultistepScheduler.from_config(config, use_karras=True, algorithm_type=\"sde-dpmsolver++\"),\n",
        "    \"Euler\": lambda config: EulerDiscreteScheduler.from_config(config),\n",
        "}\n",
        "\n",
        "# @title Function Definitions\n",
        "def inference(\n",
        "    control_image: Image.Image,\n",
        "    prompt: str,\n",
        "    negative_prompt: str,\n",
        "    guidance_scale: float = 8.0,\n",
        "    controlnet_conditioning_scale: float = 1,\n",
        "    control_guidance_start: float = 1,\n",
        "    control_guidance_end: float = 1,\n",
        "    upscaler_strength: float = 0.5,\n",
        "    seed: int = -1,\n",
        "    sampler = \"DPM++ Karras SDE\",\n",
        "    progress = gr.Progress(track_tqdm=True)\n",
        "):\n",
        "    start_time = time.time()\n",
        "    start_time_struct = time.localtime(start_time)\n",
        "    start_time_formatted = time.strftime(\"%H:%M:%S\", start_time_struct)\n",
        "    print(f\"Inference started at {start_time_formatted}\")\n",
        "\n",
        "    # Generate the initial image\n",
        "    #init_image = init_pipe(prompt).images[0]\n",
        "\n",
        "    # Rest of your existing code\n",
        "    control_image_small = center_crop_resize(control_image)\n",
        "    control_image_large = center_crop_resize(control_image, (1024, 1024))\n",
        "\n",
        "    main_pipe.scheduler = SAMPLER_MAP[sampler](main_pipe.scheduler.config)\n",
        "    my_seed = random.randint(0, 2**32 - 1) if seed == -1 else seed\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(my_seed)\n",
        "\n",
        "    out = main_pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        image=control_image_small,\n",
        "        guidance_scale=float(guidance_scale),\n",
        "        controlnet_conditioning_scale=float(controlnet_conditioning_scale),\n",
        "        generator=generator,\n",
        "        control_guidance_start=float(control_guidance_start),\n",
        "        control_guidance_end=float(control_guidance_end),\n",
        "        num_inference_steps=15,\n",
        "        output_type=\"latent\"\n",
        "    )\n",
        "    upscaled_latents = upscale(out, \"nearest-exact\", 2)\n",
        "    out_image = image_pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        control_image=control_image_large,\n",
        "        image=upscaled_latents,\n",
        "        guidance_scale=float(guidance_scale),\n",
        "        generator=generator,\n",
        "        num_inference_steps=20,\n",
        "        strength=upscaler_strength,\n",
        "        control_guidance_start=float(control_guidance_start),\n",
        "        control_guidance_end=float(control_guidance_end),\n",
        "        controlnet_conditioning_scale=float(controlnet_conditioning_scale)\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    end_time_struct = time.localtime(end_time)\n",
        "    end_time_formatted = time.strftime(\"%H:%M:%S\", end_time_struct)\n",
        "    print(f\"Inference ended at {end_time_formatted}, taking {end_time-start_time}s\")\n",
        "    return out_image[\"images\"][0], gr.update(visible=True), gr.update(visible=True), my_seed\n",
        "\n",
        "def center_crop_resize(img, output_size=(512, 512)):\n",
        "    width, height = img.size\n",
        "\n",
        "    # Calculate dimensions to crop to the center\n",
        "    new_dimension = min(width, height)\n",
        "    left = (width - new_dimension)/2\n",
        "    top = (height - new_dimension)/2\n",
        "    right = (width + new_dimension)/2\n",
        "    bottom = (height + new_dimension)/2\n",
        "\n",
        "    # Crop and resize\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "    img = img.resize(output_size)\n",
        "\n",
        "    return img\n",
        "\n",
        "def common_upscale(samples, width, height, upscale_method, crop=False):\n",
        "        if crop == \"center\":\n",
        "            old_width = samples.shape[3]\n",
        "            old_height = samples.shape[2]\n",
        "            old_aspect = old_width / old_height\n",
        "            new_aspect = width / height\n",
        "            x = 0\n",
        "            y = 0\n",
        "            if old_aspect > new_aspect:\n",
        "                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)\n",
        "            elif old_aspect < new_aspect:\n",
        "                y = round((old_height - old_height * (old_aspect / new_aspect)) / 2)\n",
        "            s = samples[:,:,y:old_height-y,x:old_width-x]\n",
        "        else:\n",
        "            s = samples\n",
        "\n",
        "        return torch.nn.functional.interpolate(s, size=(height, width), mode=upscale_method)\n",
        "\n",
        "def upscale(samples, upscale_method, scale_by):\n",
        "        #s = samples.copy()\n",
        "        width = round(samples[\"images\"].shape[3] * scale_by)\n",
        "        height = round(samples[\"images\"].shape[2] * scale_by)\n",
        "        s = common_upscale(samples[\"images\"], width, height, upscale_method, \"disabled\")\n",
        "        return (s)\n",
        "\n",
        "def check_inputs(prompt: str, control_image: Image.Image):\n",
        "    if control_image is None:\n",
        "        raise gr.Error(\"Please select or upload an Input Illusion\")\n",
        "    if prompt is None or prompt == \"\":\n",
        "        raise gr.Error(\"Prompt is required\")\n",
        "\n",
        "def convert_to_pil(base64_image):\n",
        "\n",
        "    if not hasattr(processing_utils, 'decode_base64_to_image'):\n",
        "        raise AttributeError(\"processing_utils module does not have decode_base64_to_image function\")\n",
        "\n",
        "    if not isinstance(base64_image, str) or not base64_image.startswith(\"data:image\"):\n",
        "        raise ValueError(\"base64_image should be a valid base64-encoded image\")\n",
        "\n",
        "    pil_image = processing_utils.decode_base64_to_image(base64_image)\n",
        "    return pil_image\n",
        "\n",
        "\n",
        "def convert_to_base64(pil_image):\n",
        "    base64_image = processing_utils.encode_pil_to_base64(pil_image)\n",
        "    return base64_image\n",
        "\n",
        "def get_image_input(image_path):\n",
        "    # Open and return the image object\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        return img\n",
        "    except FileNotFoundError:\n",
        "        print(\"Image not found at the specified path.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None\n",
        "\n",
        "def save_image_as_file(pil_image, file_path):\n",
        "    pil_image.save(file_path)"
      ],
      "metadata": {
        "id": "j9Mkj7Z71aZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Original Image\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "8a_xDRLcLsrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Parameters (leave at default if dont know what to do)\n",
        "prompt = \"a small-town baseball game in the 1890s, very detailed painting by Norman Rockwell\" # @param {type:\"string\"}\n",
        "negative_prompt = \"low quality image,bad, bad anatomy, too many limbs, too many fingers, too many legs, malformed\" # @param {type:\"string\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:50, step:0.25}\n",
        "controlnet_conditioning_scale = 1 # @param {type:\"slider\", min:0, max:5, step:0.05}\n",
        "control_guidance_start = 0.3 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "control_guidance_end = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "upscaler_strength = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gZJ6_GQQ4IVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Output Image\n",
        "from google.colab import files\n",
        "\n",
        "seed = random.randint(1,9999999999)\n",
        "image_path = list(uploaded.keys())[0]\n",
        "state_img_input = get_image_input(image_path)\n",
        "s = state_img_input.size\n",
        "ratio = 600/s[0]\n",
        "display(state_img_input.resize((int(s[0]*ratio), int(s[1]*ratio)), Image.ANTIALIAS))\n",
        "check_inputs(prompt,state_img_input)\n",
        "state_img_output, result_image, share_group, used_seed=inference(state_img_input,prompt, negative_prompt, guidance_scale, controlnet_conditioning_scale, control_guidance_start, control_guidance_end, upscaler_strength, seed, \"DPM++ Karras SDE\")\n",
        "result_image = convert_to_base64(state_img_output)\n",
        "pil_image = convert_to_pil(result_image)\n",
        "display(pil_image.resize((int(s[0]*ratio), int(s[1]*ratio)), Image.ANTIALIAS))\n",
        "\n",
        "save_image_as_file(pil_image,\"output.png\")\n",
        "\n",
        "files.download('output.png')"
      ],
      "metadata": {
        "id": "PFK1b4fa7hfQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}